{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8744d254",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "****************************************\n",
    " * @author: Xin Zhang\n",
    " * Date: 6/1/21\n",
    "****************************************\n",
    "\"\"\"\n",
    "import time\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "from random import sample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections.abc import Iterable\n",
    "\n",
    "activation_fcts = [\n",
    "    'relu', \"sigmoid\", \"softmax\", \"softplus\", \"softsign\", \"tanh\", \"selu\", \"elu\", \"exponential\"\n",
    "]\n",
    "optimizers = [\"sgd\", \"rmsprop\", \"adam\", \"adadelta\", \"adagrad\", \"adamax\", \"nadam\", \"ftrl\"]\n",
    "losses = [\"mae\", \"mape\", \"mse\", \"msle\", \"poisson\", \"categorical_crossentropy\"]\n",
    "paddings = [\"same\", \"valid\"]\n",
    "\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_start_time = time.time()\n",
    "        self.epoch_times = []\n",
    "        self.batch_times = []\n",
    "        self.epoch_times_detail = []\n",
    "        self.batch_times_detail = []\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        self.train_end_time = time.time()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        epoch_time_end = time.time()\n",
    "        self.epoch_times.append(epoch_time_end - self.epoch_time_start)\n",
    "        self.epoch_times_detail.append((self.epoch_time_start, epoch_time_end))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs={}):\n",
    "        self.bacth_time_start = time.time()\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        batch_time_end = time.time()\n",
    "        self.batch_times.append(batch_time_end - self.bacth_time_start)\n",
    "        self.batch_times_detail.append((self.bacth_time_start, batch_time_end))\n",
    "\n",
    "    def relative_by_train_start(self):\n",
    "        self.epoch_times_detail = np.array(self.epoch_times_detail) - self.train_start_time\n",
    "        self.batch_times_detail = np.array(self.batch_times_detail) - self.train_start_time\n",
    "        self.train_end_time = np.array(self.train_end_time) - self.train_start_time\n",
    "\n",
    "\n",
    "class ModelBuild:\n",
    "    def __init__(\n",
    "        self,\n",
    "        DEFAULT_INPUT_SHAPE=(32, 32, 3),\n",
    "        filter_lower=1,\n",
    "        filter_upper=101,\n",
    "        paddings=None,\n",
    "        dense_lower=1,\n",
    "        dense_upper=1001,\n",
    "        activations=None,\n",
    "        optimizers=None,\n",
    "        losses=None\n",
    "    ):\n",
    "        self.kwargs_list: list\n",
    "        self.layer_orders: list\n",
    "        self.DEFAULT_INPUT_SHAPE = DEFAULT_INPUT_SHAPE\n",
    "        self.activation_fcts = activation_fcts\n",
    "        self.optimizers = optimizers\n",
    "        self.losses = losses\n",
    "        self.paddings = paddings\n",
    "\n",
    "        OPTIONS = collections.defaultdict(dict)\n",
    "\n",
    "        OPTIONS[\"Model\"][\"layer\"] = [\n",
    "            \"Conv2D\", \"Dense\", \"MaxPooling2D\", \"Dropout\", \"Flatten\"\n",
    "        ]  # the model's layer can be either Conv2D or Dense\n",
    "        OPTIONS[\"Compile\"][\"optimizer\"\n",
    "                           ] = optimizers if optimizers is not None else self.optimizers.copy()\n",
    "        OPTIONS[\"Compile\"][\"loss\"] = losses if losses is not None else self.losses.copy()\n",
    "        OPTIONS[\"Dense\"][\"units\"] = list(range(dense_lower, dense_upper))\n",
    "        OPTIONS[\"Dense\"][\"activation\"\n",
    "                         ] = activations if activations is not None else self.activation_fcts.copy()\n",
    "        OPTIONS[\"Conv2D\"][\"filters\"] = list(range(filter_lower, filter_upper))\n",
    "        OPTIONS[\"Conv2D\"][\"padding\"] = paddings if paddings is not None else self.paddings.copy()\n",
    "        OPTIONS[\"Conv2D\"][\n",
    "            \"activation\"] = activations if activations is not None else self.activation_fcts.copy()\n",
    "        OPTIONS[\"MaxPooling2D\"][\"padding\"\n",
    "                                ] = paddings if paddings is not None else self.paddings.copy()\n",
    "        OPTIONS[\"Dropout\"][\"rate\"] = [0.1]\n",
    "\n",
    "        self.options = OPTIONS\n",
    "\n",
    "    def chooseRandomComb(self, options_layer, activations=None):\n",
    "        res = dict()\n",
    "        for k, v in options_layer.items():\n",
    "            if k == \"activation\" and activations is not None:\n",
    "                res[k] = random.choice(activations)\n",
    "            else:\n",
    "                res[k] = (random.sample(v, 1)[0])\n",
    "        return res\n",
    "\n",
    "    def generateRandomModelConfigList(self, layer_orders, input_shape=None):\n",
    "        \"\"\"\n",
    "        Use global variable all_comb to generate random cnn model conf\n",
    "        To build a model, pass the return to buildCnnModel method\n",
    "        \"\"\"\n",
    "        if input_shape is None:\n",
    "            input_shape = self.DEFAULT_INPUT_SHAPE\n",
    "\n",
    "        def updateImageShape(_l, _kwargs, _image_shape):\n",
    "            kernel_size: tuple\n",
    "            if _l == \"Conv2D\":\n",
    "                if type(_kwargs[\"kernel_size\"]) == int:  # when kwargs[\"kernel_size\"] was set by int\n",
    "                    kernel_size = (_kwargs[\"kernel_size\"], _kwargs[\"kernel_size\"])\n",
    "                else:\n",
    "                    kernel_size = _kwargs[\"kernel_size\"]\n",
    "            elif _l == \"MaxPooling2D\":\n",
    "                if type(_kwargs[\"pool_size\"]) == int:  # when kwargs[\"kernel_size\"] was set by int\n",
    "                    # for program simplicity, I called pool_size as kernel_size\n",
    "                    kernel_size = (_kwargs[\"pool_size\"], _kwargs[\"pool_size\"])\n",
    "                else:\n",
    "                    kernel_size = _kwargs[\"pool_size\"]\n",
    "\n",
    "            if type(_kwargs[\"strides\"]) == int:  # when kwargs[\"strides\"] was set by int\n",
    "                strides = (_kwargs[\"strides\"], _kwargs[\"strides\"])\n",
    "            else:\n",
    "                strides = _kwargs[\"strides\"]\n",
    "            if _kwargs[\"padding\"] == \"valid\":\n",
    "                _image_shape[0] = (_image_shape[0] - kernel_size[0]) // strides[0] + 1\n",
    "                _image_shape[1] = (_image_shape[1] - kernel_size[1]) // strides[1] + 1\n",
    "            if _kwargs[\"padding\"] == \"same\":\n",
    "                if _image_shape[0] % strides[0] > 0:\n",
    "                    _image_shape[0] = _image_shape[0] // strides[0] + 1\n",
    "                else:\n",
    "                    _image_shape[0] = _image_shape[0] // strides[0]\n",
    "                if _image_shape[1] % strides[1] > 0:\n",
    "                    _image_shape[1] = _image_shape[1] // strides[1] + 1\n",
    "                else:\n",
    "                    _image_shape[1] = _image_shape[1] // strides[1]\n",
    "            assert _image_shape[0] > 0 and _image_shape[1] > 0\n",
    "            return _image_shape\n",
    "\n",
    "        def validKernelStridesSize(_l, _kwargs, _image_shape):\n",
    "            if _l == \"Conv2D\":\n",
    "                if type(_kwargs[\"kernel_size\"]) == int:\n",
    "                    kernel_size = (_kwargs[\"kernel_size\"], _kwargs[\"kernel_size\"])\n",
    "                else:\n",
    "                    kernel_size = _kwargs[\"kernel_size\"]\n",
    "            elif _l == \"MaxPooling2D\":\n",
    "                if type(_kwargs[\"pool_size\"]) == int:  # when kwargs[\"kernel_size\"] was set by int\n",
    "                    # for program simplicity, I called pool_size as kernel_size\n",
    "                    kernel_size = (_kwargs[\"pool_size\"], _kwargs[\"pool_size\"])\n",
    "                else:\n",
    "                    kernel_size = _kwargs[\"pool_size\"]\n",
    "\n",
    "            if type(_kwargs[\"strides\"]) == int:\n",
    "                strides = (_kwargs[\"strides\"], _kwargs[\"strides\"])\n",
    "            else:\n",
    "                strides = _kwargs[\"strides\"]\n",
    "            judge = True\n",
    "            if _l in [\"Conv2D\", \"MaxPooling2D\"]:\n",
    "                judge = judge and (\n",
    "                    kernel_size[0] <= _image_shape[0] and kernel_size[1] <= _image_shape[1]\n",
    "                )\n",
    "            judge = judge and (strides[0] <= _image_shape[0] and strides[1] <= _image_shape[1])\n",
    "            if judge:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        options = self.options\n",
    "        kwargs_list = []\n",
    "        image_shape: list = list(input_shape[:2])\n",
    "        image_shape_list: list = []\n",
    "        # image_shape should end up in the same shape as model\n",
    "        new_layer_orders = []\n",
    "        max_strides = [3, 3]\n",
    "\n",
    "        for i, lo in enumerate(layer_orders):\n",
    "            if lo == \"Dense\":\n",
    "                kwargs = self.chooseRandomComb(options[\"Dense\"], options[\"Dense\"]['activation'])\n",
    "            elif lo == \"Conv2D\":\n",
    "                if image_shape[0] == 1 or image_shape[1] == 1:\n",
    "                    # if one of the image dim has only size one, we stop adding new conv2D\n",
    "                    continue\n",
    "                options_conv2d = options[\"Conv2D\"].copy()\n",
    "                # always ensure the kernel and strides size is smaller than the image\n",
    "                options_conv2d[\"kernel_size\"] = list(\n",
    "                    zip(range(1, image_shape[0]), range(1, image_shape[1]))\n",
    "                )\n",
    "\n",
    "                options_conv2d[\"strides\"] = [(1, 1)] * 10 + list(\n",
    "                    zip(range(1, max_strides[0]), range(1, max_strides[1]))\n",
    "                )\n",
    "                kwargs = self.chooseRandomComb(options_conv2d)\n",
    "                image_shape = updateImageShape(lo, kwargs, image_shape)\n",
    "                max_strides = [\n",
    "                    min(max_strides[0], max(1, image_shape[0])),\n",
    "                    min(max_strides[1], max(1, image_shape[1]))\n",
    "                ]\n",
    "            elif lo == \"MaxPooling2D\":\n",
    "                if image_shape[0] == 1 or image_shape[1] == 1:\n",
    "                    # if one of the image dim has only size one, we stop adding new conv2D\n",
    "                    continue\n",
    "                options_maxpooling2d = options[\"MaxPooling2D\"].copy()\n",
    "                options_maxpooling2d[\"pool_size\"] = list(\n",
    "                    zip(range(1, image_shape[0]), range(1, image_shape[1]))\n",
    "                )\n",
    "                options_maxpooling2d[\"strides\"] = [(1, 1)] * 10 + list(\n",
    "                    zip(range(1, max_strides[0]), range(1, max_strides[1]))\n",
    "                )\n",
    "                kwargs = self.chooseRandomComb(options_maxpooling2d)\n",
    "                image_shape = updateImageShape(lo, kwargs, image_shape)\n",
    "                max_strides = [\n",
    "                    min(max_strides[0], max(1, image_shape[0])),\n",
    "                    min(max_strides[1], max(1, image_shape[1]))\n",
    "                ]\n",
    "            elif lo == \"Dropout\":\n",
    "                kwargs = self.chooseRandomComb(options[\"Dropout\"])\n",
    "            elif lo == \"Flatten\":\n",
    "                kwargs = {}\n",
    "            # elif l == \"AveragePooling2D\":\n",
    "            #   pass\n",
    "            else:\n",
    "                print(\"Error: layer order contained unsupported layer: %s\" % lo)\n",
    "            kwargs_list.append(kwargs)\n",
    "            new_layer_orders.append(lo)\n",
    "            image_shape_list.append(image_shape.copy())\n",
    "\n",
    "        kwargs = {}\n",
    "        for k in [\"Compile\", \"Fit\"]:\n",
    "            kwargs[k] = {}\n",
    "            for item in options[k].keys():\n",
    "                kwargs[k][item] = random.sample(options[k][item], 1)[0]\n",
    "        kwargs_list.append(kwargs)\n",
    "        return kwargs_list, new_layer_orders, image_shape_list\n",
    "\n",
    "\n",
    "class CnnRules:\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_layer_num_lower=1,\n",
    "        conv_layer_num_upper=11,\n",
    "        max_pooling_prob=0.5,\n",
    "        dense_layer_num_lower=1,\n",
    "        dense_layer_num_upper=6\n",
    "    ):\n",
    "        self.conv_layer_num_lower = conv_layer_num_lower  # Rule: No Convolutional Layer After the First Dense Layer\n",
    "        self.conv_layer_num_upper = conv_layer_num_upper\n",
    "        self.max_pooling_prob = max_pooling_prob\n",
    "        self.dense_layer_num_lower = dense_layer_num_lower\n",
    "        self.dense_layer_num_upper = dense_layer_num_upper\n",
    "\n",
    "    def gen_cnn_rule(self):\n",
    "        conv_layer_num = np.random.randint(self.conv_layer_num_lower, self.conv_layer_num_upper)\n",
    "        dense_layer_num = np.random.randint(self.dense_layer_num_lower, self.dense_layer_num_upper)\n",
    "\n",
    "        rule_list = []\n",
    "        for _ in range(conv_layer_num):\n",
    "            rule_list.append('Conv2D')\n",
    "            max_pooling_appear = np.random.choice([True, False],\n",
    "                                                  size=1,\n",
    "                                                  replace=True,\n",
    "                                                  p=[\n",
    "                                                      self.max_pooling_prob,\n",
    "                                                      1 - self.max_pooling_prob\n",
    "                                                  ])[0]\n",
    "            if max_pooling_appear:\n",
    "                rule_list.append('MaxPooling2D')\n",
    "\n",
    "        rule_list.append('Flatten')\n",
    "\n",
    "        rule_list.extend(['Dense'] * dense_layer_num)\n",
    "\n",
    "        return rule_list\n",
    "\n",
    "\n",
    "class gen_cnn2d:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape_lower=8,\n",
    "        input_shape_upper=29,\n",
    "        conv_layer_num_lower=1,\n",
    "        conv_layer_num_upper=51,\n",
    "        filter_lower=1,\n",
    "        filter_upper=101,\n",
    "        dense_layer_num_lower=1,\n",
    "        dense_layer_num_upper=6,\n",
    "        dense_size_lower=1,\n",
    "        dense_size_upper=1001,\n",
    "        max_pooling_prob=.5,\n",
    "        input_channels=None,\n",
    "        paddings=None,\n",
    "        activations=None,\n",
    "        optimizers=None,\n",
    "        losses=None\n",
    "    ):\n",
    "        self.input_shape_lower = input_shape_lower\n",
    "        self.input_shape_upper = input_shape_upper\n",
    "        self.input_channels = input_channels if input_channels is not None else [1, 3]\n",
    "        self.conv_layer_num_lower = conv_layer_num_lower\n",
    "        self.conv_layer_num_upper = conv_layer_num_upper\n",
    "        self.filter_lower = filter_lower\n",
    "        self.filter_upper = filter_upper\n",
    "        self.dense_layer_num_lower = dense_layer_num_lower\n",
    "        self.dense_layer_num_upper = dense_layer_num_upper\n",
    "        self.dense_size_lower = dense_size_lower\n",
    "        self.dense_size_upper = dense_size_upper\n",
    "        self.max_pooling_prob = max_pooling_prob\n",
    "\n",
    "        self.activations = [\n",
    "            'relu', \"sigmoid\", \"softmax\", \"softplus\", \"softsign\", \"tanh\", \"selu\", \"elu\",\n",
    "            \"exponential\"\n",
    "        ]\n",
    "        self.optimizers = [\n",
    "            \"sgd\", \"rmsprop\", \"adam\", \"adadelta\", \"adagrad\", \"adamax\", \"nadam\", \"ftrl\"\n",
    "        ]\n",
    "        self.losses = [\"mae\", \"mape\", \"mse\", \"msle\", \"poisson\", \"categorical_crossentropy\"]\n",
    "        self.paddings = [\"same\", \"valid\"]\n",
    "\n",
    "        self.activation_pick = activations if activations is not None else self.activations.copy()\n",
    "        self.optimizer_pick = optimizers if optimizers is not None else self.optimizers.copy()\n",
    "        self.loss_pick = losses if losses is not None else self.losses.copy()\n",
    "        self.padding_pick = paddings if paddings is not None else self.paddings.copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def nothing(x):\n",
    "        return x\n",
    "\n",
    "    def generate_cnn2d_model(self):\n",
    "        cnn_rules = CnnRules(\n",
    "            conv_layer_num_lower=self.conv_layer_num_lower,\n",
    "            conv_layer_num_upper=self.conv_layer_num_upper,\n",
    "            max_pooling_prob=self.max_pooling_prob,\n",
    "            dense_layer_num_lower=self.dense_layer_num_lower,\n",
    "            dense_layer_num_upper=self.dense_layer_num_upper\n",
    "        )\n",
    "        layer_orders = cnn_rules.gen_cnn_rule()\n",
    "        input_shape = np.random.randint(self.input_shape_lower, self.input_shape_upper)\n",
    "        input_channels = np.random.choice(self.input_channels, 1)[0]\n",
    "        mb = ModelBuild(\n",
    "            DEFAULT_INPUT_SHAPE=(input_shape, input_shape, input_channels),\n",
    "            filter_lower=self.filter_lower,\n",
    "            filter_upper=self.filter_upper,\n",
    "            paddings=self.padding_pick,\n",
    "            dense_lower=self.dense_size_lower,\n",
    "            dense_upper=self.dense_size_upper,\n",
    "            activations=self.activation_pick,\n",
    "            optimizers=self.optimizer_pick,\n",
    "            losses=self.loss_pick\n",
    "        )\n",
    "        kwargs_list, layer_orders, image_shape_list = mb.generateRandomModelConfigList(layer_orders)\n",
    "        return kwargs_list, layer_orders, (int(input_shape), int(input_shape), int(input_channels))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_cnn2d_model(kwargs_list, layer_orders):\n",
    "        cnn2d = Sequential()\n",
    "        for i, lo in enumerate(layer_orders):\n",
    "            kwargs = kwargs_list[i]\n",
    "            if lo == \"Dense\":\n",
    "                cnn2d.add(Dense(**kwargs))\n",
    "            elif lo == \"Conv2D\":\n",
    "                cnn2d.add(Conv2D(**kwargs))\n",
    "            elif lo == \"MaxPooling2D\":\n",
    "                cnn2d.add(MaxPooling2D(**kwargs))\n",
    "            elif lo == \"Dropout\":\n",
    "                cnn2d.add(Dropout(**kwargs))\n",
    "            elif lo == \"Flatten\":\n",
    "                cnn2d.add(Flatten())\n",
    "        kwargs = kwargs_list[-1]\n",
    "        cnn2d.compile(metrics=['accuracy'], **kwargs[\"Compile\"])\n",
    "        return cnn2d\n",
    "\n",
    "    def generate_model_configs(self, num_model_data=1000, progress=True):\n",
    "        model_configs = []\n",
    "        if progress:\n",
    "            loop_fun = tqdm\n",
    "        else:\n",
    "            loop_fun = gen_cnn2d.nothing\n",
    "        for i in loop_fun(range(num_model_data)):\n",
    "            kwargs_list, layer_orders, input_shape = self.generate_cnn2d_model()\n",
    "            model_configs.append([kwargs_list, layer_orders, input_shape])\n",
    "        return model_configs\n",
    "\n",
    "\n",
    "class cnn2d_model_train_data:\n",
    "    def __init__(\n",
    "        self, model_configs, batch_sizes=None, epochs=None, truncate_from=None, trials=None\n",
    "    ):\n",
    "        self.model_configs = []\n",
    "        for info_list in model_configs:\n",
    "            self.model_configs.append(info_list.copy())\n",
    "        self.batch_sizes = batch_sizes if batch_sizes is not None else [2**i for i in range(1, 9)]\n",
    "        self.epochs = epochs if epochs is not None else 10\n",
    "        self.truncate_from = truncate_from if truncate_from is not None else 2\n",
    "        self.trials = trials if trials is not None else 5\n",
    "        self.activation_fcts = activation_fcts\n",
    "        self.optimizers = optimizers\n",
    "        self.losses = losses\n",
    "\n",
    "    def get_train_data(self, progress=True):\n",
    "        model_data = []\n",
    "        model_configs = []\n",
    "        if progress:\n",
    "            loop_fun = tqdm\n",
    "        else:\n",
    "            loop_fun = gen_cnn2d.nothing\n",
    "        for info_list in self.model_configs:\n",
    "            model_configs.append(info_list.copy())\n",
    "        for model_config_list in loop_fun(model_configs):\n",
    "            kwargs_list = model_config_list[0]\n",
    "            layer_orders = model_config_list[1]\n",
    "            input_shape = model_config_list[2]\n",
    "            model = gen_cnn2d.build_cnn2d_model(kwargs_list, layer_orders)\n",
    "            batch_size = sample(self.batch_sizes, 1)[0]\n",
    "            batch_size_data_batch = []\n",
    "            batch_size_data_epoch = []\n",
    "            out_shape = model.get_config()['layers'][-1]['config']['units']\n",
    "            x = np.ones((batch_size, *input_shape), dtype=np.float32)\n",
    "            y = np.ones((batch_size, out_shape), dtype=np.float32)\n",
    "            for _ in range(self.trials):\n",
    "                time_callback = TimeHistory()\n",
    "                model.fit(\n",
    "                    x,\n",
    "                    y,\n",
    "                    epochs=self.epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[time_callback],\n",
    "                    verbose=False\n",
    "                )\n",
    "                times_batch = np.array(time_callback.batch_times) * 1000\n",
    "                times_epoch = np.array(time_callback.epoch_times) * 1000\n",
    "                batch_size_data_batch.extend(times_batch)\n",
    "                batch_size_data_epoch.extend(times_epoch)\n",
    "\n",
    "            batch_times_truncated = batch_size_data_batch[self.truncate_from:]\n",
    "            epoch_times_trancuted = batch_size_data_epoch[self.truncate_from:]\n",
    "            recovered_time = [\n",
    "                np.median(batch_times_truncated)\n",
    "            ] * self.truncate_from + batch_times_truncated\n",
    "\n",
    "            model_config_list.append({\n",
    "                'batch_size': batch_size,\n",
    "                'batch_time': np.median(batch_times_truncated),\n",
    "                'epoch_time': np.median(epoch_times_trancuted),\n",
    "                'setup_time': np.sum(batch_size_data_batch) - sum(recovered_time),\n",
    "                'input_dim': input_shape\n",
    "            })\n",
    "            model_data.append(model_config_list)\n",
    "        return model_data\n",
    "\n",
    "    def convert_config_data(\n",
    "        self, model_data, max_layer_num=105, num_fill_na=0, name_fill_na=None, min_max_scaler=True\n",
    "    ):\n",
    "\n",
    "        feature_columns = [\n",
    "            'layer_type', 'layer_size', 'kernel_size', 'strides', 'padding', 'activation',\n",
    "            'optimizer', 'loss', 'batch_size', 'input_shape', 'channels'\n",
    "        ]\n",
    "        time_columns = ['batch_time', 'epoch_time', 'setup_time']\n",
    "        feature_layer_types = ['Conv2D', 'MaxPooling2D', 'Dense']\n",
    "\n",
    "        row_num = max([\n",
    "            len(activation_fcts),\n",
    "            len(optimizers),\n",
    "            len(losses),\n",
    "            len(paddings),\n",
    "            len(feature_layer_types)\n",
    "        ])\n",
    "        pos_dict = dict((i, feature_columns.index(i)) for i in feature_columns)\n",
    "        values_dict = {\n",
    "            'activation': activation_fcts,\n",
    "            'optimizer': optimizers,\n",
    "            'loss': losses,\n",
    "            'padding': paddings,\n",
    "            'layer_type': feature_layer_types\n",
    "        }\n",
    "        empty_rows = [[None] * len(feature_columns)] * row_num\n",
    "        empty_rows = [i[:] for i in empty_rows]  # break connection for lists\n",
    "        for v_type, v_list in values_dict.items():\n",
    "            for index, value in enumerate(v_list):\n",
    "                empty_rows[index][pos_dict[v_type]] = value\n",
    "\n",
    "        model_data_dfs = []\n",
    "        time_rows = []\n",
    "        for model_info in tqdm(model_data):\n",
    "            data_rows = []\n",
    "            kwargs_list = model_info[0]\n",
    "            layer_orders = model_info[1]\n",
    "            input_shape = model_info[2][0]\n",
    "            channels = model_info[2][-1]\n",
    "            train_times = model_info[3]\n",
    "            for index, layer_type in enumerate(layer_orders):\n",
    "                values = kwargs_list[index]\n",
    "                if layer_type == 'Conv2D':\n",
    "                    data_rows.append([\n",
    "                        layer_type, values['filters'], values['kernel_size'][0],\n",
    "                        values['strides'][0], values['padding'], values['activation'],\n",
    "                        kwargs_list[-1]['Compile']['optimizer'], kwargs_list[-1]['Compile']['loss'],\n",
    "                        train_times['batch_size'], input_shape, channels\n",
    "                    ])\n",
    "                elif layer_type == 'MaxPooling2D':\n",
    "                    data_rows.append([\n",
    "                        layer_type, num_fill_na, values['pool_size'][0], values['strides'][0],\n",
    "                        values['padding'], name_fill_na, kwargs_list[-1]['Compile']['optimizer'],\n",
    "                        kwargs_list[-1]['Compile']['loss'], train_times['batch_size'], input_shape,\n",
    "                        channels\n",
    "                    ])\n",
    "                elif layer_type == 'Dense':\n",
    "                    data_rows.append([\n",
    "                        layer_type, values['units'], num_fill_na, num_fill_na, name_fill_na,\n",
    "                        values['activation'], kwargs_list[-1]['Compile']['optimizer'],\n",
    "                        kwargs_list[-1]['Compile']['loss'], train_times['batch_size'], input_shape,\n",
    "                        channels\n",
    "                    ])\n",
    "                else:\n",
    "                    pass\n",
    "            time_rows.append([\n",
    "                train_times['batch_time'], train_times['epoch_time'], train_times['setup_time']\n",
    "            ])\n",
    "            data_rows.extend(empty_rows)\n",
    "            temp_df = pd.DataFrame(data_rows, columns=feature_columns)\n",
    "\n",
    "            temp_df = pd.get_dummies(temp_df)\n",
    "            temp_df = temp_df.drop(temp_df.index.tolist()[-len(empty_rows):])\n",
    "\n",
    "            columns_count = len(temp_df.columns)\n",
    "            zero_rows = np.zeros((max_layer_num, columns_count))\n",
    "            temp_array = temp_df.to_numpy()\n",
    "            temp_array = np.append(temp_array, zero_rows, 0)\n",
    "            temp_array = temp_array[:max_layer_num, ]\n",
    "            temp_df = pd.DataFrame(temp_array, columns=temp_df.columns)\n",
    "            model_data_dfs.append(temp_df)\n",
    "        time_df = pd.DataFrame(time_rows, columns=time_columns)\n",
    "        if min_max_scaler:\n",
    "            scaled_model_dfs = []\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(pd.concat(model_data_dfs, axis=0).to_numpy())\n",
    "            for data_df in model_data_dfs:\n",
    "                scaled_data = scaler.transform(data_df.to_numpy())\n",
    "                scaled_temp_df = pd.DataFrame(scaled_data, columns=temp_df.columns)\n",
    "                scaled_model_dfs.append(scaled_temp_df)\n",
    "            return scaled_model_dfs, time_df, scaler\n",
    "        return model_data_dfs, time_df, None\n",
    "\n",
    "\n",
    "class convert_cnn2d_data:\n",
    "    def __init__(self):\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "        unique_all_optimizers = sorted(list(set(self.optimizers)))\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        x_opts = [[i] for i in unique_all_optimizers]\n",
    "        enc.fit(x_opts)\n",
    "        self.enc = enc\n",
    "\n",
    "    @staticmethod\n",
    "    # for valid padding\n",
    "    def valid_padding_output(input_size, kernel_size, stride):\n",
    "        pos = kernel_size\n",
    "        output = 1\n",
    "        while True:\n",
    "            pos += stride\n",
    "            output += 1\n",
    "            if pos + stride > input_size:\n",
    "                break\n",
    "        padding = -(input_size - pos)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    # for same padding\n",
    "    def same_padding_output(input_size, kernel_size, stride):\n",
    "        if stride == 1:\n",
    "            return input_size\n",
    "        else:\n",
    "            pos = 1\n",
    "            output = 1\n",
    "            while True:\n",
    "                pos += stride\n",
    "                output += 1\n",
    "                if pos + stride > input_size:\n",
    "                    break\n",
    "            padding = pos + kernel_size - 1 - input_size\n",
    "            return output\n",
    "\n",
    "    @staticmethod\n",
    "    def conv2d_layer_flops(h, w, c, k, out):\n",
    "        return h * w * (2 * c * k * k - 1) * out\n",
    "\n",
    "    @staticmethod\n",
    "    def dense_layer_flops(i, o):\n",
    "        return (2 * i - 1) * o\n",
    "\n",
    "    @staticmethod\n",
    "    def get_flops_conv2d_keras(input_shape, conv_model_obj, sum_all=True, add_pooling=True):\n",
    "        conv_flops = []\n",
    "        pool_flops = []\n",
    "        dense_flops = []\n",
    "        all_flops = []\n",
    "        for idx, layer_data in enumerate(conv_model_obj.get_config()['layers']):\n",
    "            layer_name = layer_data['class_name']\n",
    "            layer_config = layer_data['config']\n",
    "            if layer_name == 'Conv2D' or layer_name == 'SeparableConv2D':\n",
    "                filters = layer_config['filters']\n",
    "                kernel_size = layer_config['kernel_size'][0]\n",
    "                strides = layer_config['strides'][0]\n",
    "                padding_method = layer_config['padding']\n",
    "                previous_channels = input_shape[-1]\n",
    "                if padding_method == 'same':\n",
    "                    output = convert_cnn2d_data.same_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    flops = convert_cnn2d_data.conv2d_layer_flops(\n",
    "                        input_shape[0], input_shape[1], previous_channels, kernel_size, filters\n",
    "                    )\n",
    "                    conv_flops.append(flops)\n",
    "                    all_flops.append(flops)\n",
    "                    input_shape = [output, output, filters]\n",
    "                    # conv_flops.append(np.prod(input_shape))\n",
    "                else:\n",
    "                    output = convert_cnn2d_data.valid_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    flops = convert_cnn2d_data.conv2d_layer_flops(\n",
    "                        input_shape[0], input_shape[1], previous_channels, kernel_size, filters\n",
    "                    )\n",
    "                    conv_flops.append(flops)\n",
    "                    all_flops.append(flops)\n",
    "                    input_shape = [output, output, filters]\n",
    "                    # conv_flops.append(np.prod(input_shape))\n",
    "\n",
    "            if layer_name == 'MaxPooling2D' or layer_name == 'AveragePooling2D':\n",
    "                kernel_size = layer_config['pool_size'][0]\n",
    "                strides = layer_config['strides'][0]\n",
    "                padding_method = layer_config['padding']\n",
    "                previous_channels = input_shape[-1]\n",
    "                if padding_method == 'same':\n",
    "                    output = convert_cnn2d_data.same_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    flops = convert_cnn2d_data.conv2d_layer_flops(\n",
    "                        input_shape[0], input_shape[1], previous_channels, kernel_size,\n",
    "                        previous_channels\n",
    "                    )\n",
    "                    # flops = np.prod(input_shape)\n",
    "                    pool_flops.append(flops)\n",
    "                    all_flops.append(flops)\n",
    "                    input_shape = [output, output, previous_channels]\n",
    "                else:\n",
    "                    output = convert_cnn2d_data.valid_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    flops = convert_cnn2d_data.conv2d_layer_flops(\n",
    "                        input_shape[0], input_shape[1], previous_channels, kernel_size,\n",
    "                        previous_channels\n",
    "                    )\n",
    "                    # flops = np.prod(input_shape)\n",
    "                    pool_flops.append(flops)\n",
    "                    all_flops.append(flops)\n",
    "                    input_shape = [output, output, previous_channels]\n",
    "\n",
    "            if layer_name == 'ZeroPadding2D':\n",
    "                w_padding_size = layer_config['padding'][0]\n",
    "                h_padding_size = layer_config['padding'][1]\n",
    "                input_shape = [\n",
    "                    input_shape[0] + np.sum(w_padding_size),\n",
    "                    input_shape[1] + np.sum(h_padding_size), input_shape[-1]\n",
    "                ]\n",
    "            if layer_name == 'Cropping2D':\n",
    "                w_cropping_size = layer_config['cropping'][0]\n",
    "                h_cropping_size = layer_config['cropping'][1]\n",
    "                input_shape = [\n",
    "                    input_shape[0] - np.sum(w_cropping_size),\n",
    "                    input_shape[1] - np.sum(h_cropping_size), input_shape[-1]\n",
    "                ]\n",
    "\n",
    "            if layer_name == 'Dense':\n",
    "                if isinstance(input_shape, Iterable):\n",
    "                    input_shape = np.prod(input_shape)\n",
    "                else:\n",
    "                    pass\n",
    "                flops = convert_cnn2d_data.dense_layer_flops(input_shape, layer_config['units'])\n",
    "                input_shape = layer_config['units']\n",
    "                dense_flops.append(flops)\n",
    "                all_flops.append(flops)\n",
    "                # dense_flops.append(input_shape)\n",
    "        if sum_all:\n",
    "            if add_pooling:\n",
    "                return sum(all_flops)\n",
    "            else:\n",
    "                return sum(conv_flops + dense_flops)\n",
    "        else:\n",
    "            if add_pooling:\n",
    "                return all_flops\n",
    "            else:\n",
    "                return conv_flops + dense_flops\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_shape_flow_conv2d_keras(\n",
    "        input_shape, conv_model_obj, start_from=1, up_to=3, conv_weight=1, pool_weight=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Will use the image shape flow inside the conv2d model as data\n",
    "        @param input_shape:\n",
    "        @param conv_model_obj:\n",
    "        @param start_from:\n",
    "        @param up_to:\n",
    "        @param conv_weight:\n",
    "        @param pool_weight:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        multiplications = []\n",
    "        shape_flow = []\n",
    "        dense_shapes = []\n",
    "        input_shape = conv_weight * np.array(input_shape[start_from:up_to])\n",
    "        shape_flow.append(input_shape)\n",
    "        conv_shape_flow = []\n",
    "        polling_shape_flow = []\n",
    "        conv_shape_flow.append(input_shape)\n",
    "        for idx, layer_data in enumerate(conv_model_obj.get_config()['layers']):\n",
    "            layer_name = layer_data['class_name']\n",
    "            layer_config = layer_data['config']\n",
    "            if layer_name == 'Conv2D' or layer_name == 'SeparableConv2D':\n",
    "                filters = layer_config['filters']\n",
    "                kernel_size = layer_config['kernel_size'][0]\n",
    "                strides = layer_config['strides'][0]\n",
    "                padding_method = layer_config['padding']\n",
    "                previous_channels = input_shape[-1]\n",
    "                if padding_method == 'same':\n",
    "                    output = convert_cnn2d_data.same_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    input_shape = [output, output, filters]\n",
    "                    conv_shape_flow.append(conv_weight * np.array(input_shape[start_from:up_to]))\n",
    "                    shape_flow.append(conv_weight * np.array(input_shape[start_from:up_to]))\n",
    "                    muls = kernel_size * kernel_size * previous_channels * output * output\n",
    "                    multiplications.append(muls)\n",
    "                else:\n",
    "                    output = convert_cnn2d_data.valid_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    input_shape = [output, output, filters]\n",
    "                    conv_shape_flow.append(conv_weight * np.array(input_shape[start_from:up_to]))\n",
    "                    shape_flow.append(conv_weight * np.array(input_shape[start_from:up_to]))\n",
    "                    muls = kernel_size * kernel_size * previous_channels * output * output\n",
    "                    multiplications.append(muls)\n",
    "            if layer_name == 'MaxPooling2D' or layer_name == 'AveragePooling2D':\n",
    "                kernel_size = layer_config['pool_size'][0]\n",
    "                strides = layer_config['strides'][0]\n",
    "                padding_method = layer_config['padding']\n",
    "                if padding_method == 'same':\n",
    "                    output = convert_cnn2d_data.same_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    input_shape = [output, output, input_shape[-1]]\n",
    "                    polling_shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "                    shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "                else:\n",
    "                    output = convert_cnn2d_data.valid_padding_output(\n",
    "                        input_shape[0], kernel_size, strides\n",
    "                    )\n",
    "                    input_shape = [output, output, input_shape[-1]]\n",
    "                    polling_shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "                    shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "            if layer_name == 'ZeroPadding2D':\n",
    "                w_padding_size = layer_config['padding'][0]\n",
    "                h_padding_size = layer_config['padding'][1]\n",
    "                input_shape = [\n",
    "                    input_shape[0] + np.sum(w_padding_size),\n",
    "                    input_shape[1] + np.sum(h_padding_size), input_shape[-1]\n",
    "                ]\n",
    "                polling_shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "                shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "            if layer_name == 'Cropping2D':\n",
    "                w_cropping_size = layer_config['cropping'][0]\n",
    "                h_cropping_size = layer_config['cropping'][1]\n",
    "                input_shape = [\n",
    "                    input_shape[0] - np.sum(w_cropping_size),\n",
    "                    input_shape[1] - np.sum(h_cropping_size), input_shape[-1]\n",
    "                ]\n",
    "                polling_shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "                shape_flow.append(pool_weight * np.array(input_shape[start_from:up_to]))\n",
    "\n",
    "            if layer_name == 'Dense':\n",
    "                dense_shapes.append(layer_config['units'])\n",
    "        return shape_flow, conv_shape_flow, polling_shape_flow, dense_shapes, multiplications\n",
    "\n",
    "    @staticmethod\n",
    "    def get_flops_tensorflow_graph2(model):\n",
    "        concrete = tf.function(lambda inputs: model(inputs))\n",
    "        concrete_func = concrete.get_concrete_function([\n",
    "            tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs\n",
    "        ])\n",
    "        frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            tf.graph_util.import_graph_def(graph_def, name='')\n",
    "            run_meta = tf.compat.v1.RunMetadata()\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "            flops = tf.compat.v1.profiler.profile(\n",
    "                graph=graph, run_meta=run_meta, cmd=\"op\", options=opts\n",
    "            )\n",
    "            return flops.total_float_ops\n",
    "\n",
    "    @staticmethod\n",
    "    def get_flops_tensorflow_graph(model, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = 1\n",
    "\n",
    "        real_model = tf.function(model).get_concrete_function(\n",
    "            tf.TensorSpec([batch_size] + model.inputs[0].shape[1:], model.inputs[0].dtype)\n",
    "        )\n",
    "        frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(real_model)\n",
    "\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=frozen_func.graph, run_meta=run_meta, cmd='op', options=opts\n",
    "        )\n",
    "        return flops.total_float_ops\n",
    "\n",
    "    def convert_model_config(\n",
    "        self, model_config_conv2d, layer_num_upper=105, data_type='FLOPs', min_max_scaler=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        @param model_config_conv2d:\n",
    "        @param layer_num_upper: max number of layer data want to keep, if model layers lees than the number, padding with 0\n",
    "        @param data_type: str \"FLOPs\" or \"Shape_Flow\"\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        shape_flow_data = []\n",
    "        flops_data_conv2d_layer = []\n",
    "        times_data_conv2d = []\n",
    "        all_optimizers = []\n",
    "        all_batch_sizes = []\n",
    "        for index, model_config in enumerate(tqdm(model_config_conv2d)):\n",
    "            batch_size = model_config[-1]['batch_size']\n",
    "            optimizer = model_config[0][-1]['Compile']['optimizer']\n",
    "            conv_model = gen_cnn2d.build_cnn2d_model(model_config[0], model_config[1])\n",
    "            input_shape = model_config[-2]\n",
    "            # conv_model.build(input_shape=(batch_size, *input_shape))\n",
    "            # flops = get_flops(conv_model, batch_size=batch_size)\n",
    "            shape_flow, conv_shape_flow, polling_shape_flow, dense_shapes, multiplications = convert_cnn2d_data.get_data_shape_flow_conv2d_keras(\n",
    "                input_shape, conv_model\n",
    "            )\n",
    "            shape_flow = [np.prod(i) for i in shape_flow]\n",
    "            shape_flow = shape_flow[:layer_num_upper]\n",
    "            short_position1 = layer_num_upper - len(shape_flow)\n",
    "            shape_flow = shape_flow + [0] * short_position1\n",
    "\n",
    "            flops_layer = convert_cnn2d_data.get_flops_conv2d_keras(input_shape, conv_model, False)\n",
    "            flops_layer = flops_layer[:layer_num_upper]\n",
    "            short_position = layer_num_upper - len(flops_layer)\n",
    "            flops_layer = flops_layer + [0] * short_position\n",
    "\n",
    "            flops_data_conv2d_layer.append(flops_layer)\n",
    "            shape_flow_data.append(shape_flow)\n",
    "            all_optimizers.append(optimizer)\n",
    "            all_batch_sizes.append(batch_size)\n",
    "            times_data_conv2d.append(model_config[-1]['batch_time'])\n",
    "\n",
    "        conv_data = []\n",
    "        if data_type.lower().startswith('f'):\n",
    "            model_computation_data = flops_data_conv2d_layer.copy()\n",
    "        elif data_type.lower().startswith('s'):\n",
    "            model_computation_data = shape_flow_data.copy()\n",
    "        else:\n",
    "            model_computation_data = flops_data_conv2d_layer.copy()\n",
    "\n",
    "        for size, batch, opt in tqdm(list(zip(model_computation_data, all_batch_sizes,\n",
    "                                              all_optimizers))):\n",
    "            optimizer_onehot = list(self.enc.transform([[opt]]).toarray()[0])\n",
    "            conv_data.append(size + [batch] + optimizer_onehot)\n",
    "\n",
    "        if min_max_scaler:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(conv_data)\n",
    "            scaler_conv_data = scaler.transform(conv_data)\n",
    "\n",
    "            return scaler_conv_data, np.array(times_data_conv2d), scaler\n",
    "        else:\n",
    "            return conv_data, np.array(times_data_conv2d), None\n",
    "\n",
    "    def convert_model_keras(\n",
    "        self,\n",
    "        conv_model_obj,\n",
    "        input_shape,\n",
    "        optimizer,\n",
    "        batch_size,\n",
    "        layer_num_upper=105,\n",
    "        data_type='FLOPs',\n",
    "        scaler=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        @param conv_model_obj:\n",
    "        @param input_shape: list of 3 int (height, width, channels)\n",
    "        @param optimizer:\n",
    "        @param batch_size:\n",
    "        @param layer_num_upper:\n",
    "        @param data_type: FLOPs or ShapeFlow\n",
    "        @param scaler:  None or should from convert_model_config, but if not None, also need optimizer and batch_size not None\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        shape_flow, conv_shape_flow, polling_shape_flow, dense_shapes, multiplications = convert_cnn2d_data.get_data_shape_flow_conv2d_keras(\n",
    "            input_shape, conv_model_obj\n",
    "        )\n",
    "        shape_flow = [np.prod(i) for i in shape_flow]\n",
    "        shape_flow = shape_flow[:layer_num_upper]\n",
    "        short_position1 = layer_num_upper - len(shape_flow)\n",
    "        shape_flow = shape_flow + [0] * short_position1\n",
    "\n",
    "        flops_layer = convert_cnn2d_data.get_flops_conv2d_keras(input_shape, conv_model_obj, False)\n",
    "        flops_layer = flops_layer[:layer_num_upper]\n",
    "        short_position = layer_num_upper - len(flops_layer)\n",
    "        flops_layer = flops_layer + [0] * short_position\n",
    "\n",
    "        if data_type.lower().startswith('f'):\n",
    "            layer_data = flops_layer.copy()\n",
    "        elif data_type.lower().startswith('s'):\n",
    "            layer_data = shape_flow.copy()\n",
    "        else:\n",
    "            layer_data = flops_layer.copy()\n",
    "\n",
    "        optimizer_onehot = list(self.enc.transform([[optimizer]]).toarray()[0])\n",
    "        layer_data = layer_data + [batch_size] + optimizer_onehot\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaled_data = scaler.transform(np.array([layer_data]))\n",
    "            return scaled_data\n",
    "        else:\n",
    "            return layer_data\n",
    "\n",
    "\n",
    "def demo_depreciated():\n",
    "    save_step = 100\n",
    "    data_points = 10000\n",
    "\n",
    "    split_indices = list(\n",
    "        nltk.bigrams([0] + [\n",
    "            v + index * save_step\n",
    "            for index, v in enumerate([save_step] *\n",
    "                                      (data_points // save_step) + [data_points % save_step])\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    gen = gen_cnn2d(\n",
    "        input_shape_lower=20,\n",
    "        input_shape_upper=101,\n",
    "        conv_layer_num_lower=1,\n",
    "        conv_layer_num_upper=51,\n",
    "        filter_lower=1,\n",
    "        filter_upper=101,\n",
    "        dense_layer_num_lower=1,\n",
    "        dense_layer_num_upper=6,\n",
    "        dense_size_lower=1,\n",
    "        dense_size_upper=1001,\n",
    "        max_pooling_prob=.5,\n",
    "        input_channels=None,\n",
    "        paddings=None,\n",
    "        activations=None,\n",
    "        optimizers=None,\n",
    "        losses=None\n",
    "    )\n",
    "    model_configs = gen.generate_model_configs(num_model_data=data_points, progress=True)\n",
    "\n",
    "    # Save generated data for every 100 data points\n",
    "    for start, end in tqdm(split_indices):\n",
    "        model_configs_partial = model_configs[start:end]\n",
    "    mtd = cnn2d_model_train_data(\n",
    "        model_configs_partial, batch_sizes=None, epochs=None, truncate_from=None, trials=None\n",
    "    )\n",
    "    model_data = mtd.get_train_data(progress=False)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    file_name = f'/home/jupyter/TrainDataCurrentCNN/{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.json'\n",
    "    with open(f'{file_name}', 'w') as fp:\n",
    "        json.dump(model_data, fp)\n",
    "    print(f'{end} data points saved!')\n",
    "\n",
    "    # Load raw dta\n",
    "    all_training_data_file = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(\"TrainDataCurrentCNN\"):\n",
    "        for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "            all_training_data_file.append(os.path.join(dirpath, filename))\n",
    "\n",
    "    model_data = []\n",
    "    for name in all_training_data_file:\n",
    "        with open(name, 'r') as fp:\n",
    "            model_data.extend(json.load(fp))\n",
    "\n",
    "    # 105 from conv_layer_num_upper * 2 + dense_layer_num_upper\n",
    "    # * 2 because the maxpooling layer might be there\n",
    "\n",
    "    model_data_dfs, time_df, scaler = mtd.convert_config_data(\n",
    "        model_data, max_layer_num=105, num_fill_na=0, name_fill_na=None, min_max_scaler=True\n",
    "    )\n",
    "\n",
    "    x = np.array([\n",
    "        data_df.to_numpy().reshape(\n",
    "            model_data_dfs[0].shape[0] * model_data_dfs[0].shape[1],\n",
    "        ) for data_df in model_data_dfs\n",
    "    ])\n",
    "    y = np.array(time_df.batch_time.tolist())\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    use_batchNormalization = False\n",
    "\n",
    "    if use_batchNormalization:\n",
    "        batch_model = Sequential()\n",
    "        batch_model.add(\n",
    "            Dense(2000, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu')\n",
    "        )\n",
    "        batch_model.add(BatchNormalization())\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(BatchNormalization())\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(BatchNormalization())\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(BatchNormalization())\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(BatchNormalization())\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(BatchNormalization())\n",
    "        batch_model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # Compile model\n",
    "        batch_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    else:\n",
    "        batch_model = Sequential()\n",
    "        batch_model.add(\n",
    "            Dense(2000, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu')\n",
    "        )\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "        batch_model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # Compile model\n",
    "        batch_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    history_batch = batch_model.fit(\n",
    "        x_train, y_train, batch_size=16, epochs=20, validation_data=(x_test, y_test), verbose=True\n",
    "    )\n",
    "    # summarize history for loss\n",
    "    plt.plot(history_batch.history['loss'])\n",
    "    plt.plot(history_batch.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    batch_y_pred = batch_model.predict(x_test)\n",
    "    batch_y_pred = batch_y_pred.reshape(batch_y_pred.shape[0], )\n",
    "    plt.scatter(batch_y_pred, y_test)\n",
    "\n",
    "\n",
    "def demo_new():\n",
    "    save_step = 100\n",
    "    data_points = 10000\n",
    "\n",
    "    split_indices = list(\n",
    "        nltk.bigrams([0] + [\n",
    "            v + index * save_step\n",
    "            for index, v in enumerate([save_step] *\n",
    "                                      (data_points // save_step) + [data_points % save_step])\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    gen = gen_cnn2d(\n",
    "        input_shape_lower=20,\n",
    "        input_shape_upper=101,\n",
    "        conv_layer_num_lower=1,\n",
    "        conv_layer_num_upper=51,\n",
    "        filter_lower=1,\n",
    "        filter_upper=101,\n",
    "        dense_layer_num_lower=1,\n",
    "        dense_layer_num_upper=6,\n",
    "        dense_size_lower=1,\n",
    "        dense_size_upper=1001,\n",
    "        max_pooling_prob=.5,\n",
    "        input_channels=None,\n",
    "        paddings=None,\n",
    "        activations=None,\n",
    "        optimizers=None,\n",
    "        losses=None\n",
    "    )\n",
    "    model_configs = gen.generate_model_configs(num_model_data=data_points, progress=True)\n",
    "\n",
    "    # Save generated data for every 100 data points\n",
    "    for start, end in tqdm(split_indices):\n",
    "        model_configs_partial = model_configs[start:end]\n",
    "    mtd = cnn2d_model_train_data(\n",
    "        model_configs_partial, batch_sizes=None, epochs=None, truncate_from=None, trials=None\n",
    "    )\n",
    "    model_data = mtd.get_train_data(progress=False)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    file_name = f'/home/jupyter/TrainDataCurrentCNN/{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.json'\n",
    "    with open(f'{file_name}', 'w') as fp:\n",
    "        json.dump(model_data, fp)\n",
    "    print(f'{end} data points saved!')\n",
    "\n",
    "    # Load raw dta\n",
    "    all_training_data_file_conv2d = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(\"TrainDataCurrentCNN\"):\n",
    "        for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "            all_training_data_file_conv2d.append(os.path.join(dirpath, filename))\n",
    "\n",
    "    model_data_conv2d = []\n",
    "    for name in all_training_data_file_conv2d:\n",
    "        with open(name, 'r') as fp:\n",
    "            model_data_conv2d.extend(json.load(fp))\n",
    "\n",
    "    ccd = convert_cnn2d_data()\n",
    "\n",
    "    # Convert raw data into data points\n",
    "    scaler_conv_data, times_data_conv2d, scaler = ccd.convert_model_config(\n",
    "        model_data_conv2d, layer_num_upper=105, data_type='FLOPs', min_max_scaler=True\n",
    "    )\n",
    "\n",
    "    import tensorflow.keras as keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, \\\n",
    "        BatchNormalization\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    def cal_score(pred, real, absolute=False):\n",
    "        pred = np.array(pred).copy()\n",
    "        real = np.array(real).copy()\n",
    "        if absolute:\n",
    "            return abs((pred - real) / real)\n",
    "        else:\n",
    "            return (pred - real) / real\n",
    "\n",
    "    # train data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        scaler_conv_data, times_data_conv2d, test_size=0.1, random_state=0\n",
    "    )\n",
    "\n",
    "    batch_model = keras.Sequential()\n",
    "    batch_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "    batch_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "    batch_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "    batch_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "    batch_model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "    batch_model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    batch_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    history_batch = batch_model.fit(\n",
    "        x_train, y_train, batch_size=16, epochs=15, validation_data=(x_test, y_test), verbose=True\n",
    "    )\n",
    "    batch_y_pred = batch_model.predict(x_test)\n",
    "    batch_y_pred = batch_y_pred.reshape(batch_y_pred.shape[0], )\n",
    "    plt.scatter(y_test, batch_y_pred)\n",
    "    plt.scatter(y_test, y_test, c='r')\n",
    "    plt.title(f'{np.mean(cal_score(y_test, batch_y_pred, True))}')\n",
    "    plt.show()\n",
    "\n",
    "    # convert keras model\n",
    "\n",
    "    import tensorflow.keras as keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, \\\n",
    "        BatchNormalization\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tqdm import tqdm\n",
    "    np.random.seed(1000)\n",
    "\n",
    "    # Instantiation\n",
    "\n",
    "    def build_AlexNet(input_shape=(256, 28, 28)):\n",
    "\n",
    "        AlexNet = Sequential()\n",
    "\n",
    "        # 1st Convolutional Layer\n",
    "        AlexNet.add(\n",
    "            Conv2D(\n",
    "                filters=96,\n",
    "                input_shape=input_shape,\n",
    "                kernel_size=(11, 11),\n",
    "                strides=(4, 4),\n",
    "                padding='same'\n",
    "            )\n",
    "        )\n",
    "        AlexNet.add(Activation('relu'))\n",
    "        AlexNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "        # 2nd Convolutional Layer\n",
    "        AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding='same'))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "        AlexNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "        # 3rd Convolutional Layer\n",
    "        AlexNet.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "\n",
    "        # 4th Convolutional Layer\n",
    "        AlexNet.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "\n",
    "        # 5th Convolutional Layer\n",
    "        AlexNet.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "        AlexNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "        # Passing it to a Fully Connected layer\n",
    "        AlexNet.add(Flatten())\n",
    "        # 1st Fully Connected Layer\n",
    "        AlexNet.add(Dense(4096, input_shape=(\n",
    "            32,\n",
    "            32,\n",
    "            3,\n",
    "        )))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "        # Add Dropout to prevent overfitting\n",
    "        AlexNet.add(Dropout(0.4))\n",
    "\n",
    "        # 2nd Fully Connected Layer\n",
    "        AlexNet.add(Dense(4096))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "        # Add Dropout\n",
    "        AlexNet.add(Dropout(0.4))\n",
    "\n",
    "        # 3rd Fully Connected Layer\n",
    "        AlexNet.add(Dense(1000))\n",
    "        AlexNet.add(Activation('relu'))\n",
    "        # Add Dropout\n",
    "        AlexNet.add(Dropout(0.4))\n",
    "\n",
    "        # Output Layer\n",
    "        AlexNet.add(Dense(10))\n",
    "        AlexNet.add(Activation('softmax'))\n",
    "\n",
    "        return AlexNet\n",
    "\n",
    "    AlexNet = build_AlexNet(input_shape=(256, 28, 28))\n",
    "    AlexNet_data = ccd.convert_model_keras(\n",
    "        AlexNet, (256, 28, 28), 'sgd', 128, layer_num_upper=105, data_type='FLOPs', scaler=scaler\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08eaf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current memory usage 7.2%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import psutil\n",
    "print(f'current memory usage {psutil.virtual_memory().percent}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6392746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2021/10/02 12:20:47.724',\n",
       " 'driver_version': '460.73.01',\n",
       " 'count': '1',\n",
       " 'name': 'Tesla P100-PCIE-16GB',\n",
       " 'pcie.link.width.max': '16',\n",
       " 'vbios_version': '86.00.52.00.02',\n",
       " 'memory.total [MiB]': '16280 MiB',\n",
       " 'temperature.gpu': '42'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_trainingtime_prediction.env_detect import gpu_features, general_features\n",
    "\n",
    "gpu_features().get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fbf498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_features = general_features(install_lshw=True).get_features()\n",
    "# other_features['cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca3a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_features['memory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53151380",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_step = 5\n",
    "data_points = 10000\n",
    "\n",
    "\n",
    "split_indices = list(\n",
    "    nltk.bigrams([0] + [\n",
    "        v + index * save_step\n",
    "        for index, v in enumerate([save_step] *\n",
    "                                  (data_points // save_step) + [data_points % save_step])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba99fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:23<00:00, 428.80it/s]\n"
     ]
    }
   ],
   "source": [
    "gen = gen_cnn2d(input_shape_lower=40,\n",
    "        input_shape_upper=129,\n",
    "        conv_layer_num_lower=1,\n",
    "        conv_layer_num_upper=51,\n",
    "        filter_lower=1,\n",
    "        filter_upper=101,\n",
    "        dense_layer_num_lower=1,\n",
    "        dense_layer_num_upper=6,\n",
    "        dense_size_lower=1,\n",
    "        dense_size_upper=1001,\n",
    "        max_pooling_prob=1,\n",
    "        input_channels=None,\n",
    "        paddings=None,\n",
    "        activations=None,\n",
    "        optimizers=None,\n",
    "        losses=None)\n",
    "model_configs = gen.generate_model_configs(num_model_data=data_points, progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2001 [00:41<23:00:50, 41.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2001 [02:53<52:29:33, 94.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2001 [03:41<40:44:19, 73.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2001 [06:45<65:02:37, 117.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2001 [07:17<47:59:04, 86.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2001 [07:55<38:47:52, 70.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2001 [27:35<239:44:01, 432.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2001 [29:27<183:00:32, 330.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2001 [30:49<139:52:36, 252.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2001 [31:45<106:12:34, 192.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/2001 [32:25<80:32:49, 145.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/2001 [33:00<61:47:26, 111.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/2001 [34:09<54:37:46, 98.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 data points saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/2001 [39:51<95:07:12, 172.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 data points saved!\n"
     ]
    }
   ],
   "source": [
    "for start, end in tqdm(split_indices):\n",
    "    model_configs_partial = model_configs[start: end]\n",
    "    mtd = cnn2d_model_train_data(model_configs_partial,\n",
    "                       batch_sizes=None,\n",
    "                       epochs=None,\n",
    "                       truncate_from=None,\n",
    "                       trials=None)\n",
    "    model_data = mtd.get_train_data(progress=False)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    file_name = f'/home/jupyter/TrainDataCurrentCNN/{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.json'\n",
    "    with open(f'{file_name}', 'w') as fp:\n",
    "        json.dump(model_data, fp)\n",
    "    print(f'{end} data points saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_data_file = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(\"TrainDataCurrentCNN\"):\n",
    "    for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "        all_training_data_file.append(os.path.join(dirpath, filename))\n",
    "\n",
    "model_data = []\n",
    "for name in all_training_data_file:\n",
    "    with open(name, 'r') as fp:\n",
    "        model_data.extend(json.load(fp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa11ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721237eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 105 from conv_layer_num_upper * 2 + dense_layer_num_upper\n",
    "# * 2 because the maxpooling layer might be there\n",
    "\n",
    "model_data_dfs, time_df, scaler = mtd.convert_config_data(model_data, \n",
    "                                                          max_layer_num=105, \n",
    "                                                          num_fill_na=0, \n",
    "                                                          name_fill_na=None, \n",
    "                                                          min_max_scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3341fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([data_df.to_numpy().reshape(model_data_dfs[0].shape[0]*model_data_dfs[0].shape[1],) for data_df in model_data_dfs])\n",
    "y = np.array(time_df.batch_time.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697253ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e064d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_batchNormalization = False \n",
    "\n",
    "if use_batchNormalization:\n",
    "  batch_model = Sequential()\n",
    "  batch_model.add(Dense(2000, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(BatchNormalization())\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(BatchNormalization())\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(BatchNormalization())\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(BatchNormalization())\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(BatchNormalization())\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(BatchNormalization())\n",
    "  batch_model.add(Dense(1, kernel_initializer='normal'))\n",
    "  # Compile model\n",
    "  batch_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "else:\n",
    "  batch_model = Sequential()\n",
    "  batch_model.add(Dense(2000, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(Dense(2000, kernel_initializer='normal', activation='relu'))\n",
    "  batch_model.add(Dense(1, kernel_initializer='normal'))\n",
    "  # Compile model\n",
    "  batch_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history_batch = batch_model.fit(x_train, y_train, batch_size=16, epochs=20, validation_data=(x_test, y_test), verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history_batch.history['loss'])\n",
    "plt.plot(history_batch.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f506f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_pred = batch_model.predict(x_test)\n",
    "batch_y_pred = batch_y_pred.reshape(batch_y_pred.shape[0],)\n",
    "plt.scatter(batch_y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xin_util.ZipAndUnzip import zip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file('TrainDataCurrentCNN', 'TrainDataCurrentCNN.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1632ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
